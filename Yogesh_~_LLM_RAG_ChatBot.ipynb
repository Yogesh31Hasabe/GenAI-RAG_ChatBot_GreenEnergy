{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setup of OpenAI API & Model"
      ],
      "metadata": {
        "id": "7x02TE_9VKHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUrY5vAWU-sV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "current_date = datetime.datetime.now().date()\n",
        "if current_date < datetime.date(2023, 9, 2):\n",
        "    llm_name = \"gpt-3.5-turbo-0301\"\n",
        "else:\n",
        "    llm_name = \"gpt-3.5-turbo\"\n",
        "print(llm_name)"
      ],
      "metadata": {
        "id": "WLWVSda-VDjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading and Data Splitting"
      ],
      "metadata": {
        "id": "xMtLAo1CVZWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "folder_path = \"dataset_docs/green_energy_pdfs/\"\n",
        "\n",
        "# Load PDF\n",
        "loaders = []\n",
        "# Iterate over all files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(folder_path, filename)\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        loaders.append(loader)\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "print(f\"Total no. of PDF files are : {len(loaders)}. \\nTotal no. of pages of all PDF files are : {len(docs)}.\")"
      ],
      "metadata": {
        "id": "FJFnWpi1VYmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap = 24\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(f\"Total no. of Chunks created after splitting are : {len(splits)}.\")"
      ],
      "metadata": {
        "id": "90awUnBvVi3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Embeddings for Data Chunks and Performing VectorStores"
      ],
      "metadata": {
        "id": "grkKEfAYcaLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embedding = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "SCwg2H7KcibN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "persist_directory = 'dataset_docs/chroma/'"
      ],
      "metadata": {
        "id": "0mQKu0CIcjv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf ./dataset_docs/chroma  # remove old database files if any"
      ],
      "metadata": {
        "id": "fQqpMa7RcqpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "print(f\"Total no. of Collections stored in Chroma VectorDB are : {vectordb._collection.count()}.\")"
      ],
      "metadata": {
        "id": "arWFSUmhc3C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrival QA Chain creation\n",
        "\n",
        "#### Create conversation chain that uses our vectordb as retriver, this also allows for chat history management"
      ],
      "metadata": {
        "id": "8gNO1nhydpmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
        "retriever=vectordb.as_retriever()\n",
        "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)"
      ],
      "metadata": {
        "id": "pn3KtqlKgQQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting all the RAG components together and creating a ChatBot Interface"
      ],
      "metadata": {
        "id": "uncJIjAeg3bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Initialize chat history\n",
        "chat_history = []\n",
        "\n",
        "# Function to simulate chatbot interaction\n",
        "def chat():\n",
        "    print(\"Welcome to AI chatbot! Type 'exit' to stop.\")\n",
        "    while True:\n",
        "        query = input(\"Please type here: \")\n",
        "\n",
        "        if query.lower() == 'exit':\n",
        "            print(\"Thank you for using the AI chatbot!\")\n",
        "            break\n",
        "\n",
        "        # Simulate the chatbot's response\n",
        "        result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "        chat_history.append((query, result['answer']))\n",
        "\n",
        "        # Display the conversation\n",
        "        display(HTML(f'<b><font color=\"blue\">Human_User:</font></b> {query}'))\n",
        "        display(HTML(f'<b><font color=\"green\">AI_Chatbot:</font></b> {result[\"answer\"]}'))\n",
        "\n",
        "# Run the chat function\n",
        "chat()"
      ],
      "metadata": {
        "id": "TBJfGV3Yn8uD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}